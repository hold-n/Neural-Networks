{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n"
     ]
    }
   ],
   "source": [
    "train_path = pathlib.Path('../blobs/notMNIST_large')\n",
    "test_path = pathlib.Path('../blobs/notMNIST_small')\n",
    "\n",
    "labels = np.array([item.name for item in train_path.glob('*')])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train_files, y_train = [], []\n",
    "x_test_files, y_test = [], []\n",
    "for label, index in tqdm(zip(labels, range(len(labels)))):\n",
    "    train_dir = train_path / label\n",
    "    files = pd.Series(train_dir / name for name in os.listdir(train_dir))\n",
    "    files = files.sample(len(files) // 10)\n",
    "    x_train_files.extend(files)\n",
    "    y_train.extend(index for _ in range(len(files)))\n",
    "    \n",
    "    test_dir = test_path / label\n",
    "    test_files = [test_dir / name for name in os.listdir(test_dir)]\n",
    "    x_test_files.extend(test_files)\n",
    "    y_test.extend(index for _ in range(len(test_files)))\n",
    "    \n",
    "x_train_files, y_train = sklearn.utils.shuffle(np.array(x_train_files), np.array(y_train))\n",
    "x_test_files, x_val_files, y_test, y_val = train_test_split(x_test_files, y_test, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    img = keras.preprocessing.image.load_img(path, color_mode='grayscale')\n",
    "    return keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "x_train = np.array([load_img(p) for p in x_train_files])\n",
    "x_val = np.array([load_img(p) for p in x_val_files])\n",
    "x_test = np.array([load_img(p) for p in x_test_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 43166\n"
     ]
    }
   ],
   "source": [
    "# TODO: add regularization?\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=8, kernel_size=(4, 4), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='sgd', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print('# of parameters:', np.sum([keras.backend.count_params(w) for w in model.trainable_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "51696/51696 [==============================] - 16s 304us/sample - loss: 0.2674 - acc: 0.9241\n",
      "Epoch 2/10\n",
      "51696/51696 [==============================] - 16s 307us/sample - loss: 0.2622 - acc: 0.9248\n",
      "Epoch 3/10\n",
      "51696/51696 [==============================] - 17s 321us/sample - loss: 0.2573 - acc: 0.9255\n",
      "Epoch 4/10\n",
      "51696/51696 [==============================] - 16s 308us/sample - loss: 0.2510 - acc: 0.9280\n",
      "Epoch 5/10\n",
      "51696/51696 [==============================] - 18s 343us/sample - loss: 0.2471 - acc: 0.9285\n",
      "Epoch 6/10\n",
      "51696/51696 [==============================] - 17s 320us/sample - loss: 0.2425 - acc: 0.9302\n",
      "Epoch 7/10\n",
      "51696/51696 [==============================] - 18s 344us/sample - loss: 0.2383 - acc: 0.9321\n",
      "Epoch 8/10\n",
      "51696/51696 [==============================] - 18s 339us/sample - loss: 0.2330 - acc: 0.9325\n",
      "Epoch 9/10\n",
      "51696/51696 [==============================] - 18s 353us/sample - loss: 0.2289 - acc: 0.9336\n",
      "Epoch 10/10\n",
      "51696/51696 [==============================] - 18s 355us/sample - loss: 0.2237 - acc: 0.9353\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:\n",
      "9362/9362 [==============================] - 1s 74us/sample - loss: 0.3164 - acc: 0.9182\n"
     ]
    }
   ],
   "source": [
    "print('Test prediction:')\n",
    "score, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, after 30 iterations we've overfitted the test dataset. It used to be around **92.5%**. Not great, but the architecture was a wild guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 10210\n"
     ]
    }
   ],
   "source": [
    "pooling_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "pooling_model.compile(\n",
    "    optimizer='sgd', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print('# of parameters:', np.sum([keras.backend.count_params(w) for w in pooling_model.trainable_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "51696/51696 [==============================] - 6s 120us/sample - loss: 0.4535 - acc: 0.8763\n",
      "Epoch 2/10\n",
      "51696/51696 [==============================] - 6s 119us/sample - loss: 0.4498 - acc: 0.8773\n",
      "Epoch 3/10\n",
      "51696/51696 [==============================] - 7s 129us/sample - loss: 0.4466 - acc: 0.8787\n",
      "Epoch 4/10\n",
      "51696/51696 [==============================] - 7s 131us/sample - loss: 0.4431 - acc: 0.8794\n",
      "Epoch 5/10\n",
      "51696/51696 [==============================] - 7s 134us/sample - loss: 0.4400 - acc: 0.8799\n",
      "Epoch 6/10\n",
      "51696/51696 [==============================] - 7s 135us/sample - loss: 0.4372 - acc: 0.8807\n",
      "Epoch 7/10\n",
      "51696/51696 [==============================] - 7s 137us/sample - loss: 0.4346 - acc: 0.8821\n",
      "Epoch 8/10\n",
      "51696/51696 [==============================] - 7s 140us/sample - loss: 0.4322 - acc: 0.8814\n",
      "Epoch 9/10\n",
      "51696/51696 [==============================] - 8s 153us/sample - loss: 0.4297 - acc: 0.8827\n",
      "Epoch 10/10\n",
      "51696/51696 [==============================] - 8s 155us/sample - loss: 0.4276 - acc: 0.8844\n"
     ]
    }
   ],
   "source": [
    "history = pooling_model.fit(x_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:\n",
      "9362/9362 [==============================] - 1s 62us/sample - loss: 0.2530 - acc: 0.9342\n"
     ]
    }
   ],
   "source": [
    "print('Test pooling prediction:')\n",
    "score, accuracy = pooling_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling reduced the number of parameters by a factor of 4 while having greater accuracy! **93.4%** is decent\n",
    "While pooling is a useful construct this might be just because less parameters are easier to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 61226\n"
     ]
    }
   ],
   "source": [
    "lenet = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape = (28, 28, 1)),\n",
    "    keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'),\n",
    "    keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=120, kernel_size=(3, 3), activation='tanh'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(84, activation='tanh'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "lenet.compile(\n",
    "    optimizer='sgd', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print('# of parameters:', np.sum([keras.backend.count_params(w) for w in lenet.trainable_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51696/51696 [==============================] - 12s 225us/sample - loss: 1.0896 - acc: 0.7076\n",
      "Epoch 2/20\n",
      "51696/51696 [==============================] - 12s 236us/sample - loss: 0.7745 - acc: 0.7927\n",
      "Epoch 3/20\n",
      "51696/51696 [==============================] - 13s 244us/sample - loss: 0.6849 - acc: 0.8103\n",
      "Epoch 4/20\n",
      "51696/51696 [==============================] - 14s 270us/sample - loss: 0.6204 - acc: 0.8223\n",
      "Epoch 5/20\n",
      "51696/51696 [==============================] - 11s 216us/sample - loss: 0.5726 - acc: 0.8334\n",
      "Epoch 6/20\n",
      "51696/51696 [==============================] - 10s 198us/sample - loss: 0.5363 - acc: 0.8442\n",
      "Epoch 7/20\n",
      "51696/51696 [==============================] - 10s 203us/sample - loss: 0.5081 - acc: 0.8516\n",
      "Epoch 8/20\n",
      "51696/51696 [==============================] - 11s 204us/sample - loss: 0.4857 - acc: 0.8576\n",
      "Epoch 9/20\n",
      "51696/51696 [==============================] - 11s 215us/sample - loss: 0.4674 - acc: 0.8621\n",
      "Epoch 10/20\n",
      "51696/51696 [==============================] - 12s 235us/sample - loss: 0.4529 - acc: 0.8668\n",
      "Epoch 11/20\n",
      "51696/51696 [==============================] - 12s 229us/sample - loss: 0.4404 - acc: 0.8699\n",
      "Epoch 12/20\n",
      "51696/51696 [==============================] - 11s 211us/sample - loss: 0.4299 - acc: 0.8730\n",
      "Epoch 13/20\n",
      "51696/51696 [==============================] - 12s 233us/sample - loss: 0.4202 - acc: 0.8756\n",
      "Epoch 14/20\n",
      "51696/51696 [==============================] - 11s 214us/sample - loss: 0.4120 - acc: 0.8773\n",
      "Epoch 15/20\n",
      "51696/51696 [==============================] - 12s 231us/sample - loss: 0.4047 - acc: 0.8798\n",
      "Epoch 16/20\n",
      "51696/51696 [==============================] - 14s 268us/sample - loss: 0.3977 - acc: 0.8816\n",
      "Epoch 17/20\n",
      "51696/51696 [==============================] - 10s 203us/sample - loss: 0.3911 - acc: 0.8825\n",
      "Epoch 18/20\n",
      "51696/51696 [==============================] - 10s 199us/sample - loss: 0.3853 - acc: 0.8847\n",
      "Epoch 19/20\n",
      "51696/51696 [==============================] - 12s 228us/sample - loss: 0.3789 - acc: 0.8859\n",
      "Epoch 20/20\n",
      "51696/51696 [==============================] - 11s 214us/sample - loss: 0.3741 - acc: 0.8881\n"
     ]
    }
   ],
   "source": [
    "history = lenet.fit(x_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test LeNet prediction:\n",
      "9362/9362 [==============================] - 1s 100us/sample - loss: 0.2071 - acc: 0.9414\n"
     ]
    }
   ],
   "source": [
    "print('Test LeNet prediction:')\n",
    "score, accuracy = lenet.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet wins! After training for just 20 iterations we have accuracy above **94%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the showdown:\n",
    "\n",
    "1. LeNet is the obvious winner. This architecture is canonical for a reason\n",
    "2. One pooling layer performed better supposedly becaused it regularized the model\n",
    "3. Plain CNN wasn't bad, but still 3rd\n",
    "\n",
    "Note that while overall performance is not vastly greater than that of feed-forward NN from assignment 2, we did not optimize the architecture much. Even with LeNet we could'be at least trained for more epochs.\n",
    "\n",
    "Also, we still used just 1/10th of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
